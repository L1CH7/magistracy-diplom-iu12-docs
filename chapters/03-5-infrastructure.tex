\section{Инфраструктурные компоненты и среда развертывания}

Для обеспечения надежности, переносимости и наблюдаемости системы был разработан комплекс инфраструктурных компонентов, отвечающих за конфигурацию, логирование и оркестрацию контейнеров.

\subsection{Иерархическая система конфигурации}

В распределенных системах управление конфигурацией часто становится источником ошибок. Хранение настроек в коде делает систему негибкой. Использование простых JSON-файлов не позволяет переиспользовать общие блоки (например, настройки подключения к БД) между различными сервисами, что приводит к дублированию. Кроме того, необходимо обеспечить возможность переопределения параметров при развертывании в контейнерах без правки файлов кода.

В рамках проекта разработана гибридная система конфигурации на базе библиотек Pydantic, PyYAML, PyYAML-Include. Это позволяет объединить строго типизированные модели данных Python с гибкостью YAML-файлов.
Для поддержки модульности реализован кастомный загрузчик YAML, поддерживающий директиву \texttt{!include}. Это позволяет разбить монолитный конфиг на логические части:

\begin{code}{yaml-include}{YAML}{Использование директивы include в YAML (фрагмент \texttt{map.yaml})}
map_view:
  center: [55.75, 37.61]
  zoom: 12

# Вынос правил детализации в отдельный файл
lod_rules: !include map.lod.yaml

services:
  data_processor: http://data-processor:8000
\end{code}

Поверх файловой конфигурации работает механизм переопределения через переменные окружения, что соответствует методологии 12-Factor App \cite{12factor2024}. Используется префикс \texttt{APP\_\_} и двойное подчеркивание как разделитель вложенности.
Например, переменная \texttt{APP\_\_DATABASE\_\_HOST=10.0.0.5} автоматически переопределит значение \texttt{database.host} в объекте настроек.

Типизация Pydantic обеспечивает валидацию настроек на старте приложения: сервис не запустится, если вместо числа в порт передан текст. Механизм \texttt{!include} устранил дублирование конфигурации между четырьмя микросервисами. Переменные окружения позволили инъектировать секреты (пароли БД) и адреса сервисов в Docker-контейнеры, не сохраняя их в репозитории кода.

\subsection{Унифицированная система логирования}

Для реализации унифицированной системы наблюдения внедрена библиотека \textbf{loguru} \cite{loguru2019async}. Она выступает единой точкой сбора всех событий приложения, перехватывая сообщения от стандартного модуля \texttt{logging} и серверного движка Uvicorn.

Настройка логгера вынесена в отдельную утилиту \texttt{configure\_loguru}, которая инициализирует два обработчика:
\begin{enumerate}
    \item \textbf{обработчик стандартного вывода} -- обеспечивает цветовое форматирование сообщений для повышения удобства разработки и отладки;
    \item \textbf{файловый обработчик} -- осуществляет ротацию лог-файлов по достижении размера 500~МБ с последующим сжатием в ZIP-архив для оптимизации использования дискового пространства.
\end{enumerate}

Для интеграции с FastAPI и Uvicorn реализован класс \texttt{InterceptHandler}, который перенаправляет стандартные записи логов в Loguru, сохраняя контекст вызова (имя файла, номер строки):

\begin{code}{loguru-setup}{Python}{Настройка перехвата логов и ротации}
# services/common/utils/loguru\_config.py
logger.add(
    sys.stdout,
    level=log_level,
    format="<green>[{time:YYYY.MM.DD HH:mm:ss.SSS}]</green> <level>{message}</level>"
)

if log_to_file:
    logger.add(
        f"logs/{service\_name}.log",
        rotation="500 MB",
        level=log_level,
        compression="zip"
    )

class InterceptHandler(logging.Handler):
    def emit(self, record):
        # Перенаправление стандартного logging -> loguru
        logger.opt(depth=depth, exception=record.exc\_info).log(level, record.getMessage())
\end{code}

\textbf{Ключевой особенностью внедренного решения} является поддержка <<ленивых>> вычислений аргументов логирования. В отличие от стандартных форматных строк, где форматирование выражения \texttt{f"Data: \{heavy\_func()\}"} выполняется всегда (даже если уровень лога отключен), \texttt{loguru} вычисляет значения только при необходимости записи. Это позволяет оставлять в коде детальные отладочные сообщения с тяжелой сериализацией структур данных, не опасаясь деградации производительности в высоконагруженной среде.

Архитектура сбора логов построена на базе стека \textbf{PLG} (Promtail, Loki, Grafana) \cite{paskhin2022anomaly}.
\begin{itemize}
    \item \textbf{Promtail} представляет собой агент-сборщик, осуществляющий чтение потоков \texttt{stdout} и лог-файлов контейнеров.
    \item \textbf{Loki} выполняет индексацию метаданных (время, метки) и хранение логов в сжатом неструктурированном виде, что обеспечивает высокую скорость записи и компактность.
    \item \textbf{Grafana} обеспечивает интерфейс для выполнения запросов на языке LogQL и визуализации метрик на основе логов.
\end{itemize}

\subsection{Контейнеризация и оркестрация}

Ручное развертывание сложного комплекса из 4 контейнеров (Gateway, Data Processor, Router, PostGIS) вместе с клиентом сопряжено с рисками ошибок конфигурации сети и конфликтами портов \cite{caldas2021microservices}. Требуется гарантия идентичности окружения разработки и эксплуатации.

Среда развертывания описана декларативно в манифесте \texttt{docker-compose.yml} \cite{liu2025containerization, docker2024healthcheck}.
Реализована строгая изоляция сетевых сегментов:
\begin{itemize}
    \item \textbf{backend-net} является внутренней сетью для межсервисного взаимодействия, недоступной из внешних сетей;
    \item \textbf{frontend-net} представляет собой публичный сетевой сегмент, к которому подключен исключительно шлюз API Gateway (порты 80/443).
\end{itemize}

Для обеспечения целостности запуска настроены проверки жизнеспособности.
Сервис \texttt{router-service} имеет зависимость \texttt{service\_healthy} от базы данных:
\begin{code}{docker-healthcheck}{YAML}{Конфигурация проверки жизнеспособности зависимостей}
router:
  depends_on:
    db:
      condition: service_healthy
db:
  healthcheck:
    test: ["CMD-SHELL", "pg_isready -U postgres"]
    interval: 5s
    timeout: 5s
    retries: 5
\end{code}

Контейнеризация обеспечила полную воспроизводимость развертывания в любой среде, на который можно установить \texttt{docker}. Изоляция сетей повысила безопасность, исключив прямой доступ к БД из интернета. Проверки жизнеспособности предотвращают <<падение>> зависимых сервисов при старте, автоматически перезапуская их до готовности инфраструктуры \cite{simpledocker2024orchestration}.
