\section{Инфраструктурные компоненты и среда развертывания}

Для обеспечения надежности, переносимости и наблюдаемости системы был разработан комплекс инфраструктурных компонентов, отвечающих за конфигурацию, логирование и оркестрацию контейнеров.

\subsection{Иерархическая система конфигурации}

\paragraph{Проблема.}
В распределенных системах управление конфигурацией часто становится источником ошибок. Хранение настроек в коде (Hardcode) делает систему негибкой. Использование простых JSON-файлов не позволяет переиспользовать общие блоки (например, настройки подключения к БД) между различными сервисами, что приводит к дублированию. Кроме того, необходимо обеспечить возможность переопределения параметров при развертывании в контейнерах (через переменные окружения) без правки файлов.

\paragraph{Реализация.}
Разработана гибридная система конфигурации на базе библиотеки \textbf{Pydantic}. Она объединяет строго типизированные модели данных Python с гибкостью YAML-файлов.
Для поддержки модульности реализован кастомный загрузчик YAML, поддерживающий директиву \texttt{!include}. Это позволяет разбить монолитный конфиг на логические части:
\begin{code}{yaml-include}{YAML}{Использование директивы include в YAML}
database: !include common/database.yaml
logging: !include common/logging.yaml
server:
  host: 0.0.0.0
  port: 8000
\end{code}

Поверх файловой конфигурации работает механизм переопределения через переменные окружения (Environment Variables). Используется префикс \texttt{APP\_\_} и двойное подчеркивание как разделитель вложенности.
Например, переменная \texttt{APP\_\_DATABASE\_\_HOST=10.0.0.5} автоматически переопределит значение \texttt{database.host} в объекте настроек.

\paragraph{Преимущество.}
Типизация Pydantic обеспечивает валидацию настроек на старте приложения (Fail Fast): сервис не запустится, если вместо числа в порт передан текст. Механизм \texttt{!include} устранил дублирование конфигурации между четырьмя микросервисами. Переменные окружения позволили инъектировать секреты (пароли БД) и адреса сервисов в Docker-контейнеры, не сохраняя их в репозитории кода.

\subsection{Отказоустойчивое логирование (Zero-Effect Logging)}

\paragraph{Проблема.}
В высоконагруженных асинхронных приложениях операции ввода-вывода (I/O) при записи логов могут заблокировать цикл событий (Event Loop). Если диск перегружен или сетевой лог-колектор недоступен, вызов \texttt{print()} или стандартного \texttt{logging} может остановить обработку всех запросов на сотни миллисекунд.

\paragraph{Реализация.}
Внедрена библиотека \textbf{loguru}, настроенная в режиме асинхронного стока (Asynchronous Sink).
Параметр \texttt{enqueue=True} гарантирует, что запись сообщения в файл или отправка по сети выполняется в отдельном потоке (Thread), не блокируя основной цикл приложения:
\begin{code}{loguru-setup}{Python}{Настройка асинхронного стока логов}
logger.add(
    "logs/app.json",
    rotation="500 MB",
    serialize=True,  # JSON формат
    enqueue=True,    # Асинхронная запись
    compression="zip"
)
\end{code}

Также используется принцип «ленивой оценки» (Lazy Evaluation) аргументов логирования. Форматирование строки \texttt{logger.debug("Data: \{data\}", data=heavy\_object)} происходит только если уровень DEBUG включен. В продакшн-режиме (INFO) тяжелая сериализация объекта не выполняется, экономя ресурсы CPU.

\paragraph{Преимущество.}
Концепция «Zero-Effect Logging» минимизировала влияние подсистемы наблюдения на производительность. Время ответа (Latency) API практически не зависит от объема генерируемых логов. Структурированный JSON-вывод позволил автоматизировать сбор метрик в ELK Stack (Elasticsearch, Logstash, Kibana).

\subsection{Контейнеризация и оркестрация}

\paragraph{Проблема.}
Развертывание сложного комплекса из 5 контейнеров (Gateway, Data Processor, Router, PostGIS, Frontend) вручную чревато ошибками конфигурации сети ("Dependency Hell") и конфликтами портов. Требуется гарантия идентичности окружения разработки и эксплуатации.

\paragraph{Реализация.}
Среда развертывания описана декларативно в манифесте \texttt{docker-compose.yml}.
Реализована строгая изоляция сетевых контуров:
\begin{itemize}
    \item \textbf{backend-net}: Внутренняя сеть для межсервисного взаимодействия. Недоступна извне.
    \item \textbf{frontend-net}: Публичная сеть, в которую смотрит только API Gateway (порт 80/443).
\end{itemize}

Для обеспечения целостности запуска (Startup Order) настроены проверки жизнеспособности (Health-checks).
Сервис \texttt{router-service} имеет зависимость \texttt{service\_healthy} от базы данных:
\begin{code}{docker-healthcheck}{YAML}{Конфигурация проверки жизнеспособности зависимостей}
router:
  depends_on:
    db:
      condition: service_healthy
db:
  healthcheck:
    test: ["CMD-SHELL", "pg_isready -U postgres"]
    interval: 5s
    timeout: 5s
    retries: 5
\end{code}

\paragraph{Преимущество.}
Контейнеризация обеспечила полную воспроизводимость развертывания (Infrastructure as Code). Изоляция сетей повысила безопасность, исключив прямой доступ к БД из интернета. Health-checks предотвращают "падение" зависимых сервисов при старте, автоматически перезапуская их до готовности инфраструктуры.
