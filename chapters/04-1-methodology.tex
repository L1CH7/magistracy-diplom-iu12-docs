\section{Методология экспериментального исследования}

Для обеспечения достоверности и воспроизводимости результатов была разработана методология тестирования, имитирующая профиль нагрузки реальной навигационной системы в условиях мегаполиса.

\subsection{Тестовый стенд и характеристики данных}

В качестве полигона для испытаний была выбрана дорожная сеть Московской агломерации (Bounding Box: $37.36^{\circ}$E -- $37.88^{\circ}$E, $55.55^{\circ}$N -- $55.92^{\circ}$N). Выбор обусловлен высокой плотностью графа (более 250\,000 вершин и 400\,000 ребер), наличием сложных многоуровневых развязок (ТТК, МКАД) и высокой связностью (Connectivity), что создает максимальную нагрузку на алгоритмы поиска пути \cite{dlr_osm_deep_dive_2024}.

Аппаратная конфигурация сервера:
\begin{itemize}
    \item \textbf{CPU:} AMD Ryzen 5 5600H with Radeon Graphics (6 ядер, 12 потоков, 3.3-4.2 ГГц).
    \item \textbf{RAM:} 16 ГБ DDR4-3200 (выделено под Shared Buffers PostgreSQL: 4 ГБ).
    \item \textbf{Storage:} NVMe SSD (Read: 3500 MB/s).
    \item \textbf{GPU:} NVIDIA GeForce RTX 3060 Mobile (6 ГБ).
    \item \textbf{OS:} Linux Arch (Kernel 6.17.5-arch1-1).
\end{itemize}

Для генерации нагрузочного тестирования использовался специально разработанный модуль на языке \textbf{Python} с применением библиотек \texttt{pytest} и \texttt{httpx} (для асинхронных HTTP-запросов).
Нагрузочное тестирование проводилось в двух режимах:
\begin{itemize}
    \item \textbf{Latency Test (Последовательный):} 1 клиент, выполняющий запросы синхронно. Цель — измерение "чистого" времени обработки запроса маршрутизации (End-to-End Latency) без влияния очереди.
    \item \textbf{Throughput Test (Параллельный):} 16 конкурентных клиентов (воркеров), запущенных через \texttt{pytest-xdist}. Количество воркеров ($N=16$) выбрано исходя из конфигурации сервера ($cpu\_count \times 2$) для достижения максимальной утилизации ресурсов.
\end{itemize}

Вместо синтетических тестов был разработан сценарий, генерирующий случайные путевые точки (waypoints) на основе реального распределения адресов, что позволило избежать эффекта "горячего кэша". В ходе экспериментов фиксировались метрики времени отклика, длины маршрута и потребления ресурсов.

В рамках исследования проводились три группы тестов:
\begin{enumerate}
    \item \textbf{Latency Test (Тест задержки).} Оценка чистого времени выполнения запроса одним клиентом. Измеряется время от момента отправки HTTP-запроса до получения первого байта ответа (TTFB) и полного тела ответа.
    \item \textbf{Throughput Test (Тест пропускной способности).} Оценка предельной производительности системы при конкурентном доступе. Нагрузка линейно увеличивалась от 1 до 50 активных пользователей (Spawn Rate: 1 user/sec). Фиксировались метрики RPS (Requests Per Second) и процент ошибок (Fail Rate).
    \item \textbf{Cache Performance Test (Тест эффективности кэширования).} Сравнение производительности при "холодном старте" (после перезагрузки СУБД) и при "прогретом кэше" (Warmup), когда индексы и страницы данных уже находятся в оперативной памяти (OS Page Cache / PG Shared Buffers).
\end{enumerate}

Отказ от полностью синтетической генерации графа (Random Graph) в пользу реальных данных OSM обоснован необходимостью проверки корректности топологии (например, запретов поворотов и одностороннего движения), что невозможно смоделировать на искусственных структурах.
